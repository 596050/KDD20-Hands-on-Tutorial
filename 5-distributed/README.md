This section extends the mini-batch training shown in the previous section to distributed setting. It contains the parts below:

**Part 1**: introduction of the basic concept of distributed training and the components provided by DGL
([slides](https://github.com/dglai/KDD20-Hands-on-Tutorial/raw/master/5-distributed/distributed%20training%20tutorial.pptx)).
This introduction assumes the readers have read the previous section on mini-batch training.

**Part 2**: Jupyter Notebook to partition the OGB product graph
([notebook](https://github.com/dglai/KDD20-Hands-on-Tutorial/blob/master/5-distributed/partition.ipynb)).

**Part 3**: Jupyter Notebook to demonstrate the basic operations on distributed components of DGL
([notebook](https://github.com/dglai/KDD20-Hands-on-Tutorial/blob/master/5-distributed/basic.ipynb))

**Part 4**: Jupyter Notebook for distributed GraphSage for node classification
([notebook](https://github.com/dglai/KDD20-Hands-on-Tutorial/blob/master/5-distributed/Distributed%20Node%20Classification.ipynb))

**Part 5**: Jupyter Notebook for distributed GraphSage with node embeddings for node classification
([notebook](https://github.com/dglai/KDD20-Hands-on-Tutorial/blob/master/5-distributed/Distributed%20Node%20Classification-emb.ipynb))
