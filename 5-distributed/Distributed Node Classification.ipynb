{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_config = None\n",
    "conf_path = 'standalone_data/ogbn-products.json'\n",
    "num_epochs = 20\n",
    "num_hidden = 16\n",
    "num_layers = 2\n",
    "batch_size = 1000\n",
    "batch_size_eval = 100000\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "standalone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='GCN')\n",
    "parser.add_argument('--ip_config', type=str, help='The file for IP configuration')\n",
    "parser.add_argument('--conf_path', type=str, help='The path to the partition config file')\n",
    "parser.add_argument('--num-epochs', type=int, default=20)\n",
    "parser.add_argument('--num-hidden', type=int, default=16)\n",
    "parser.add_argument('--num-layers', type=int, default=2)\n",
    "parser.add_argument('--batch-size', type=int, default=1000)\n",
    "parser.add_argument('--batch-size-eval', type=int, default=100000)\n",
    "parser.add_argument('--standalone', action='store_true')\n",
    "args = parser.parse_args()\n",
    "\n",
    "ip_config = args.ip_config\n",
    "conf_path = args.conf_path\n",
    "num_epochs = args.num_epochs\n",
    "num_hidden = args.num_hidden\n",
    "num_layers = args.num_layers\n",
    "batch_size = args.batch_size\n",
    "batch_size_eval = args.batch_size_eval\n",
    "standalone = args.standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DistGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.distributed.DistGraph(ip_config, 'ogbn-products', conf_file=conf_path)\n",
    "print('#nodes:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nid = dgl.distributed.node_split(g.ndata['train_mask'])\n",
    "valid_nid = dgl.distributed.node_split(g.ndata['val_mask'])\n",
    "test_nid = dgl.distributed.node_split(g.ndata['test_mask'])\n",
    "print('train set:', len(train_nid))\n",
    "print('valid set:', len(valid_nid))\n",
    "print('test set:', len(test_nid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = g.ndata['labels'][0:g.number_of_nodes()]\n",
    "uniq_labels = th.unique(labels)\n",
    "num_labels = len(uniq_labels)\n",
    "print('#labels:', num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, blocks, x):\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            x = layer(block, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "print('#features:', g.ndata['features'].shape[1])\n",
    "model = SAGE(g.ndata['features'].shape[1], num_hidden, num_labels, num_layers)\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([10, 25])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, train_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, valid_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, fanouts, sample_neighbors):\n",
    "        self.g = g\n",
    "        self.fanouts = fanouts\n",
    "        self.sample_neighbors = sample_neighbors\n",
    "\n",
    "    def sample_blocks(self, seeds):\n",
    "        seeds = th.LongTensor(np.asarray(seeds))\n",
    "        blocks = []\n",
    "        for fanout in self.fanouts:\n",
    "            # For each seed node, sample ``fanout`` neighbors.\n",
    "            frontier = self.sample_neighbors(self.g, seeds, fanout, replace=True)\n",
    "            # Then we compact the frontier into a bipartite graph for message passing.\n",
    "            block = dgl.to_block(frontier, seeds)\n",
    "            # Obtain the seed nodes for next layer.\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "\n",
    "            blocks.insert(0, block)\n",
    "        return blocks\n",
    "\n",
    "sampler = NeighborSampler(g, [10, 25], dgl.distributed.sample_neighbors)\n",
    "\n",
    "# Create PyTorch DataLoader for constructing blocks\n",
    "dataloader = DataLoader(\n",
    "        dataset=train_nid.numpy(),\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=sampler.sample_blocks,\n",
    "        shuffle=True,\n",
    "        drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(5):\n",
    "    # Loop over the dataloader to sample the computation dependency graph as a list of\n",
    "    # blocks.\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for step, blocks in enumerate(dataloader):\n",
    "        input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "        seeds = blocks[-1].dstdata[dgl.NID]\n",
    "        \n",
    "        # Load the input features as well as output labels\n",
    "        batch_inputs = g.ndata['features'][input_nodes]\n",
    "        batch_labels = g.ndata['labels'][seeds]\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        batch_pred = model(blocks, batch_inputs)\n",
    "        loss = loss_fcn(batch_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # Aggregate gradients in multiple nodes.\n",
    "        if not standalone:\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    th.distributed.all_reduce(param.grad.data,\n",
    "                                              op=th.distributed.ReduceOp.SUM)\n",
    "                    param.grad.data /= dgl.distributed.get_num_client()\n",
    "\n",
    "        optimizer.step()\n",
    "    print('epoch {}: training takes {:.3f} seconds, loss={:.3f}'.format(epoch, time.time() - start, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over the dataloader to sample the computation dependency graph as a list of\n",
    "    # blocks.\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(train_dataloader):\n",
    "        # Load the input features as well as output labels\n",
    "        batch_inputs = g.ndata['features'][input_nodes]\n",
    "        batch_labels = g.ndata['labels'][seeds]\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        batch_pred = model(blocks, batch_inputs)\n",
    "        loss = loss_fcn(batch_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # Aggregate gradients in multiple nodes.\n",
    "        if not standalone:\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    th.distributed.all_reduce(param.grad.data,\n",
    "                                              op=th.distributed.ReduceOp.SUM)\n",
    "                    param.grad.data /= dgl.distributed.get_num_client()\n",
    "\n",
    "        optimizer.step()\n",
    "    print('Epoch {}: training takes {:.3f} seconds, loss={:.3f}'.format(epoch, time.time() - start, np.mean(losses)))\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    start = time.time()\n",
    "    with th.no_grad():\n",
    "        for step, (input_nodes, seeds, blocks) in enumerate(valid_dataloader):\n",
    "            inputs = g.ndata['features'][input_nodes]\n",
    "            labels.append(g.ndata['labels'][seeds].numpy())\n",
    "            predictions.append(model(blocks, inputs).argmax(1).numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {}: validation takes {:.3f} seconds, Validation Accuracy {}'.format(epoch, time.time() - start, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = dgl.distributed.node_split(np.ones(g.number_of_nodes()),\n",
    "                                   g.get_partition_book(), force_even=True)\n",
    "y = dgl.distributed.DistTensor(g, (g.number_of_nodes(), num_hidden), th.float32)\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([None])\n",
    "test_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, nodes, sampler,\n",
    "    batch_size=10000,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "x = g.ndata['features']\n",
    "for l, layer in enumerate(model.layers):\n",
    "    if l == len(model.layers) - 1:\n",
    "        y = dgl.distributed.DistTensor(g, (g.number_of_nodes(), num_labels), th.float32)\n",
    "    for input_nodes, seeds, blocks in test_dataloader:\n",
    "        block = blocks[0]\n",
    "        h = x[input_nodes]\n",
    "        with th.no_grad():\n",
    "            h = layer(block, h)\n",
    "            if l != len(model.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "            y[seeds] = h\n",
    "    x = y\n",
    "    g.barrier()\n",
    "\n",
    "predictions = y[test_nid].argmax(1).numpy()\n",
    "labels = g.ndata['labels'][test_nid]\n",
    "accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "print('Test takes {:.3f} seconds, acc={:.3f}'.format(time.time() - start, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([10, 25])\n",
    "test_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, test_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "predictions = []\n",
    "labels = []\n",
    "with th.no_grad():\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(test_dataloader):\n",
    "        inputs = g.ndata['features'][input_nodes]\n",
    "        labels.append(g.ndata['labels'][seeds].numpy())\n",
    "        predictions.append(model(blocks, inputs).argmax(1).numpy())\n",
    "    predictions = np.concatenate(predictions)\n",
    "    labels = np.concatenate(labels)\n",
    "    accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "    print('Epoch {} Test Accuracy {}'.format(epoch, accuracy))\n",
    "print('Test takes {:.3f} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
